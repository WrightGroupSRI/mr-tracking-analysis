{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf8a626",
   "metadata": {},
   "source": [
    "# Active catheter tracking notebook: test run\n",
    "To run the full pipeline from raw data download through reconstruction and analysis requires resources that may not be available depending on your setup: about 5 GB storage and a few minutes on a single processor, with less time required for a multi-processor system. On resource-limited systems where storage and computation resources may not be available, skip to the [Analysis](#analysis) section, which uses precomputed outputs from the reconstruction and localization if the notebook-processed outputs are not available.\n",
    "\n",
    "The notebook is broken down into:\n",
    "- [Download](#download) the raw tracking projections from each experiment\n",
    "- [Localize](#localize): Reconstruct signals and localize the coils using each algorithm\n",
    "- [Verify](#verify) the results against preprocessed data\n",
    "- [Analysis](#analysis): Run the static and dynamic analysis notebooks. **SKIP TO HERE** if you don't want to download the full 5GB dataset and reconstruct the raw data on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b8991",
   "metadata": {},
   "source": [
    "# Download dataset from Zenodo <a class=\"anchor\" id=\"download\"></a>\n",
    "\n",
    "Download the raw data from the Active Tracking Dataset. For now, we download from the sandbox until the pipeline is fully tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d163673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!zenodo_get 295280 -g raw.zip --sandbox -o /data\n",
    "!wget -P /data https://sandbox.zenodo.org/records/295280/files/raw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9800661",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /data && unzip -qn raw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05eca0",
   "metadata": {},
   "source": [
    "**Warning** Download of the active tracking raw data requires approximately 5 GB of space. If you prefer, skip ahead to the analysis section which uses the precomputed outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab473f1b",
   "metadata": {},
   "source": [
    "# Run localization algorithms on raw data <a class=\"anchor\" id=\"localize\"></a>\n",
    "**Warning:** this step may be slow. To use precomputed outputs instead, skip to the analysis section.\n",
    "\n",
    "Test cathy localize:\n",
    "- run \"cathy localize\" on raw projection data: this will generate coordinate output in text files from each of the localization algorithms\n",
    "- ex/ cathy localize -d 6 -p 7 input_dir output_dir\n",
    "    - runs the algorithm for coils 6 (distal) and 7 (proximal) for the projection files under \"input_dir\"\n",
    "    - output_dir will contain subdirectories for each of the algorithms (peak, centroid, centroid_around_peak, png, jpng)\n",
    "    - each algorithm subdirectory will contain a coordinate text output file for each coil\n",
    "\n",
    "for this test:\n",
    "- input_dir: /data/raw/static/trackTest-13Dec2021-C306-Y0/1/FH512_noDither_gradSpoiled-2021-12-13T13_02_33.756\n",
    "- output_dir: /data/localize_c306-y0-1-fh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pathlib\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import shutil\n",
    "import hashlib\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c07b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_dir = os.getcwd()\n",
    "note_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b07555",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a641ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cathy localize -d 6 -p 7 /data/raw/static/trackTest-13Dec2021-C306-Y0/1/FH512_noDither_gradSpoiled-2021-12-13T13_02_33.756 /data/localize_c306-y0-1-fh "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49ddc15",
   "metadata": {},
   "source": [
    "Test the equivalent regular python method, with option to set which algorithms to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e4dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cathy.cli as cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad5d60",
   "metadata": {},
   "source": [
    "The full list of raw data directories is in the included csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5769c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings_csv = note_dir + '/data/meta/catheter_raw_recordings.csv'\n",
    "prepend = '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "procdir = prepend+'processed' # processed data should be here if download & localization work\n",
    "\n",
    "def data_processed(procdir=procdir):\n",
    "    return os.path.isdir(procdir) and len(os.listdir(procdir)) != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96210f5a",
   "metadata": {},
   "source": [
    "The below cell will reconstruct all the raw data and run the localization algorithms using a single cpu: this will be slow. On non-binder instance: Skip this and run the next two cells to distribute the work across cpus & finish faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632cb4b7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%%time\n",
    "```python\n",
    "\n",
    "directory_list = [] # list of preprocessed & processed output directory tuples\n",
    "doRun = True # True to run cathy localize: ow/ will fill in the directory list but not run cathy\n",
    "\n",
    "with open(recordings_csv,'r') as csvfile:\n",
    "    rdr = csv.DictReader(csvfile)\n",
    "    for row in rdr:\n",
    "        raw_dir = pathlib.PurePath(row['Input'])\n",
    "        source = pathlib.PurePath(prepend).joinpath(raw_dir)\n",
    "        distal = int(row['distal'])\n",
    "        proximal = int(row['proximal'])\n",
    "        dest = pathlib.PurePath(prepend).joinpath('processed').joinpath(pathlib.PurePath(*raw_dir.parts[1:]))\n",
    "        preproc = pathlib.PurePath(prepend).joinpath('preprocessed').joinpath(pathlib.PurePath(*raw_dir.parts[1:]))\n",
    "        directory_list.append( (preproc,dest))\n",
    "        print('source: ' + str(source))\n",
    "        print('dest: ' + str(dest))\n",
    "        if doRun:\n",
    "            os.makedirs(dest, exist_ok=True)\n",
    "            # !cathy localize -d {distal} -p {proximal} {source} {dest}\n",
    "            cat.run_localize(str(source), str(dest), distal, proximal, algos=['centroid_around_peak', 'jpng'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aaa38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOL = 0.05\n",
    "MAX_ITER = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fcf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_localize(args):\n",
    "    assert(len(args[0]) == 6), \"Parameter issue: \" + str(args)\n",
    "    src = args[0][0]\n",
    "    dst = args[0][1]\n",
    "    dist = args[0][2]\n",
    "    prox = args[0][3]\n",
    "    tolerance = args[0][4]\n",
    "    max_iter = args[0][5]\n",
    "    print(\"run src: \" + str(src) + \", dest: \" + str(dst) + \", distal: \" + str(dist) + \", proximal: \" + str(prox) +\n",
    "         \", tol(mm): \" + str(tolerance) + \", max iterations: \" + str(max_iter) + \"\\n\")\n",
    "    return cat.run_localize(src, dst, dist, prox, algos=['centroid_around_peak', 'jpng'],output_iterations=True,\n",
    "                           tol=tolerance, max_iterations=max_iter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# list of preprocessed & processed output directories and coil args\n",
    "experiment_data_tuples = {'static':[], 'dynamic':[], 'vivo': []}\n",
    "\n",
    "doRun = True # True to run cathy localize: ow/ will fill in the data_tuples but not run localize\n",
    "\n",
    "new_dir_list = [] #compare reorg processed directory to repo preprocessed directory\n",
    "repo_dir = note_dir + '/data/'\n",
    "\n",
    "cpu_limit_env = os.getenv(\"CPU_LIMIT\")\n",
    "if cpu_limit_env is None:\n",
    "    cpu_count = mp.cpu_count()\n",
    "else:\n",
    "    cpu_count = cpu_limit_env\n",
    "# set up the localize arguments based on the spreadsheet:\n",
    "with open(recordings_csv,'r') as csvfile:\n",
    "    rdr = csv.DictReader(csvfile)\n",
    "    for row in rdr:\n",
    "        raw_dir = pathlib.PurePath(row['Input'])\n",
    "        expmt = 'dynamic'\n",
    "        if 'static' in str(raw_dir):\n",
    "            expmt = 'static'\n",
    "        elif 'vivo' in str(raw_dir):\n",
    "            expmt = 'vivo'\n",
    "        source = pathlib.PurePath(prepend).joinpath(raw_dir)\n",
    "        distal = int(row['distal'])\n",
    "        proximal = int(row['proximal'])\n",
    "        dest = pathlib.PurePath(prepend).joinpath('processed').joinpath(pathlib.PurePath(*raw_dir.parts[1:]))\n",
    "        experiment_data_tuples[expmt].append( (str(source), dest, distal, proximal, TOL, MAX_ITER))\n",
    "        preproc = pathlib.PurePath(repo_dir).joinpath('preprocessed').joinpath(pathlib.PurePath(*raw_dir.parts[1:]))\n",
    "        if os.path.isdir(source):\n",
    "            new_dir_list.append( (preproc,dest))\n",
    "            os.makedirs(dest, exist_ok=True)\n",
    "        else:\n",
    "            raise Exception(\"Missing raw data: Please skip directly to the Analysis section near the end of this notebook\")\n",
    "\n",
    "# data_tuples contains our arguments: we can split the processing across cpus for performance\n",
    "print(\"Running localize on \" + str(len(experiment_data_tuples['static'])) + \" static directories, \" \\\n",
    "     + str(len(experiment_data_tuples['dynamic'])) + \" dynamic directories, and \" + \\\n",
    "      str(len(experiment_data_tuples['vivo'])) + \" in vivo directories\")\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "iterations_overall = {'jpng':[]} # accumulate iterations required for each recording\n",
    "\n",
    "iterations_xpmt = { 'static':{'jpng':[]}, 'dynamic':{'jpng':[]}, 'vivo':{'jpng':[]}} #separated by experiment type\n",
    "\n",
    "if doRun:\n",
    "    for xkey in experiment_data_tuples.keys():\n",
    "        print(\"Experiment: \" + xkey)\n",
    "        for result in pool.map( call_localize, [experiment_data_tuples[xkey][i:i+1] for i in range(0,len(experiment_data_tuples[xkey]))] ):\n",
    "            for key in result.keys():\n",
    "                if key in iterations_overall:\n",
    "                    iterations_overall[key] = np.concatenate((iterations_overall[key],result[key]))\n",
    "                    iterations_xpmt[xkey][key] = np.concatenate( (iterations_xpmt[xkey][key],result[key]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0beb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_processed():\n",
    "    for key in iterations_overall.keys():\n",
    "        print(f\"{key}: mean iterations {np.mean(iterations_overall[key])}\\t, median {np.median(iterations_overall[key])},\\\n",
    "        min {np.min(iterations_overall[key])}, max {np.max(iterations_overall[key])}\")\n",
    "        print(f\"percentage of iterations reaching 32: {np.count_nonzero(iterations_overall[key] == 32)/len(iterations_overall[key]) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_processed():\n",
    "    for xkey in iterations_xpmt:\n",
    "        print(f\"Experiment set: {xkey}\")\n",
    "        for key in iterations_xpmt[xkey]:\n",
    "            print(f\"\\t{key}: mean iterations {np.mean(iterations_xpmt[xkey][key])}\\t, \\\n",
    "            median {np.median(iterations_xpmt[xkey][key])},\\\n",
    "            min {np.min(iterations_xpmt[xkey][key])}, max {np.max(iterations_xpmt[xkey][key])}\")\n",
    "            print(f\"\\tpct iterations reaching 32: {np.count_nonzero(iterations_xpmt[xkey][key]==32)/len(iterations_xpmt[xkey][key]) * 100:.2f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59d898",
   "metadata": {},
   "source": [
    "# Copy ground truth data to processed directory\n",
    "Ground truth files are expected in the processed directory for later analysis. These should be copied from the raw subdirectories to the corresponding processed subdirectories. A list of these files is in meta/gt_files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5dba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_processed():\n",
    "    gtf = open(note_dir + '/data/meta/gt_files.txt','r')\n",
    "    gt_list = gtf.readlines()\n",
    "    for gt_file in gt_list:\n",
    "        gt_path = pathlib.PurePath(gt_file.rstrip())\n",
    "        src = pathlib.PurePath(prepend).joinpath(gt_path)\n",
    "        dest = pathlib.PurePath(prepend).joinpath('processed').joinpath(pathlib.PurePath(*gt_path.parts[1:]))\n",
    "        dest_dir = pathlib.PurePath(*dest.parts[:-1])\n",
    "        print('dest dir: ' + str(dest_dir))\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        print('cp ' + str(src) + ' ' + str(dest))\n",
    "        shutil.copyfile(src,dest)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c3990",
   "metadata": {},
   "source": [
    "# Verify processed outputs <a class=\"anchor\" id=\"verify\"></a>\n",
    "Check the processed outputs against the preprocessed. Initial check below will compare files in detail to ensure this method works - the original data only stored an SNR of 0 so we need to ignore this field until we can verify the rest of the data.\n",
    "\n",
    "Note we have updated the preprocessed coordinates with output using the current algorithm parameters: the next cell is redundant as we have the checksums in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac62bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#```python\n",
    "import catheter_utils.cathcoords\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "algos = ['centroid_around_peak','jpng']\n",
    "\n",
    "error_count = 0\n",
    "EPS = 0.1 #0.001\n",
    "\n",
    "def distance(c1, c2):\n",
    "    return np.linalg.norm(c2-c1)\n",
    "\n",
    "proc_file_list = [] # This will contain a list of the output files\n",
    "distances = []\n",
    "for src_test, dst_test in new_dir_list:\n",
    "    for algo in algos:\n",
    "        src_dir = pathlib.PurePath(src_test).joinpath(algo)\n",
    "        dst_dir = pathlib.PurePath(dst_test).joinpath(algo)\n",
    "        src_files = glob.glob(str(src_dir)+'/cathcoords*.txt')\n",
    "        for src in src_files:\n",
    "            srcpath = pathlib.PurePath(src)\n",
    "            fname = srcpath.parts[-1]\n",
    "            dstpath = dst_dir.joinpath(fname)\n",
    "            if (os.path.isfile(dstpath)):\n",
    "                cc_src = catheter_utils.cathcoords.read_file(srcpath)\n",
    "                #print(\"cc_src: \" + str(cc_src))\n",
    "                cc_dst = catheter_utils.cathcoords.read_file(dstpath)\n",
    "                proc_file_list.append(dstpath)\n",
    "                # Check each xyz coordinate\n",
    "                src_coords = cc_src.coords\n",
    "                dst_coords = cc_dst.coords\n",
    "                dists = list(map(distance, src_coords, dst_coords))\n",
    "                times_equal = np.array_equal(cc_src.times,cc_dst.times)\n",
    "                trigs_equal = np.array_equal(cc_src.trigs,cc_dst.trigs)\n",
    "                resps_equal = np.array_equal(cc_src.resps,cc_dst.resps)\n",
    "                if (any(i > EPS for i in dists) or not times_equal or not trigs_equal or not resps_equal):\n",
    "                    print(\"Mismatch in: \" + str(src))\n",
    "                    error_count += 1                    \n",
    "                else:\n",
    "                    print(\".\", end = \"\")\n",
    "            else:\n",
    "                print(\"Missing: \" + str(dstpath))\n",
    "                error_count += 1\n",
    "print(\"\\nErrors: \" + str(error_count))\n",
    "#```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0939479d",
   "metadata": {},
   "source": [
    "# Hash of files\n",
    "The processed files should match with the preprocessed data. A checksum file \"md5sums.txt\" was created for all the cathcoords outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_md5(filename, chunk_size=4096):\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filename, \"rb\") as f:\n",
    "        # digest = hashlib.file_digest(f, digest) Python >= 3.11 reqd!\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            md5.update(chunk)\n",
    "        return md5.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532482f6",
   "metadata": {},
   "source": [
    "## This was already run to create the md5sums file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae331cf",
   "metadata": {},
   "source": [
    "```python\n",
    "proc_md5_sum = repo_dir + 'meta/proc_md5sums.txt'\n",
    "with open(proc_md5_sum, \"w\") as f_out:\n",
    "    for proc_file in proc_file_list:\n",
    "        md5_string = generate_md5(str(proc_file))\n",
    "        f_out.write(\"{} : {}\\n\".format(str(proc_file), md5_string))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87100865",
   "metadata": {},
   "source": [
    "## Verify processed files against md5sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_md5_sum = note_dir + '/data/meta/proc_md5sums.txt'\n",
    "error_count = 0\n",
    "\n",
    "with open(preproc_md5_sum, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=':')\n",
    "    records = sum(1 for row in reader)\n",
    "    f.seek(0)\n",
    "    bar = IntProgress(min=0, max=records,description='Verifying:')\n",
    "    display(bar)\n",
    "    for row in reader:\n",
    "        proc_file = row[0].strip()\n",
    "        proc_digest = row[1].strip()\n",
    "        if (not os.path.isfile(proc_file)):\n",
    "            print('Coordinate file ' + proc_file + ' missing')\n",
    "            error_count += 1\n",
    "        else:\n",
    "            digest = generate_md5(proc_file)\n",
    "            bar.value += 1\n",
    "            if (digest != proc_digest):\n",
    "                print('Coordinate file ' + proc_file + ' mismatch')\n",
    "                error_count += 1\n",
    "\n",
    "print('\\nErrors: ' + str(error_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_limit = os.environ.get(\"CPU_LIMIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ff3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cpu_limit is None:\n",
    "    cpu_limit = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec21339",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpu_quota_within_docker():\n",
    "    cpu_cores = None\n",
    "    cfs_period = pathlib.Path(\"/sys/fs/cgroup/cpu/cpu.cfs_period_us\")\n",
    "    cfs_quota = pathlib.Path(\"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\")\n",
    "\n",
    "    if cfs_period.exists() and cfs_quota.exists():\n",
    "        with cfs_period.open('rb') as p, cfs_quota.open('rb') as q:\n",
    "            p, q = int(p.read()), int(q.read())\n",
    "            # get the cores allocated by dividing the quota\n",
    "            # in microseconds by the period in microseconds\n",
    "            cpu_cores = math.ceil(q / p) if q > 0 and p > 0 else None\n",
    "    return cpu_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cores = get_cpu_quota_within_docker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c98e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a14f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /sys/fs/cgroup/cpu/cpu.cfs_period_us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98552bb3",
   "metadata": {},
   "source": [
    "# Analysis <a class=\"anchor\" id=\"analysis\"></a>\n",
    "The notebooks are in subdirectories. We call each notebook in turn below.\n",
    "## Static Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2dbb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd Static_Tracking\n",
    "%run static_tracking_heatmaps_Y0.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c52298",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run static_tracking_heatmaps_Y1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7665c78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run static_tracking_heatmaps_Y2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036ecdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%cd ../Dynamic_Tracking\n",
    "%run dynamic_tracking_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952a8cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%cd ../Invivo_Tracking\n",
    "%run invivo_tracking_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset to main directory\n",
    "%cd /code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
